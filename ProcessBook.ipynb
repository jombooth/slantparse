{
 "metadata": {
  "name": "",
  "signature": "sha256:b20108739eea4589661fb8140afea0b062b56461cf7f904494b8a30bbb4a7208"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2 align=\"center\">slant parse: <br><br> Using Machine Learning to Determine the Political Leanings of News Publications</h2>\n",
      "\n",
      "<br><center>Serena L Booth, Harvard College, Class of 2016</center>\n",
      "\n",
      "<center>Joseph M Booth, Harvard College, Class of 2015</center>\n",
      "\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Part 0: Metric "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_of_political_magazines = ['http://en.wikipedia.org/wiki/List_of_political_magazines', 'http://en.wikipedia.org/wiki/Category:Modern_liberal_American_magazines',\n",
      "                               'http://en.wikipedia.org/wiki/Category:Conservative_American_magazines', 'http://usconservatives.about.com/od/gettinginvolved/tp/TopConservativeMagazines.htm',\n",
      "                               'http://www.allyoucanread.com/top-10-political-magazines/', 'http://www.conservapedia.com/Conservative_media', \n",
      "                               'http://www.dailykos.com/story/2009/04/05/716698/-The-Compleat-Revised-Guide-to-Liberal-and-Progressive-News-and-Politics-on-the-Web',\n",
      "                               'http://www.washingtonpost.com/blogs/the-fix/wp/2014/10/21/lets-rank-the-media-from-liberal-to-conservative-based-on-their-audiences/']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 1: Data Collection\n",
      "\n",
      "We scrape 5 liberal sources: \n",
      "    - Mother Jones\n",
      "    - The Nation\n",
      "    - Slate\n",
      "    - The New Yorker\n",
      "    - The Washington Post\n",
      "\n",
      "And 5 conservative sources:\n",
      "    - The Christian Science Monitor\n",
      "    - The Weekly Standard\n",
      "    - TownHall\n",
      "    - The American Conservative\n",
      "    - The American Spectator\n",
      "    \n",
      "These sources are added to a csv file called articles.csv. The format is: publication title, political leaning (C or L), article title, article text. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "import csv\n",
      "from bs4 import BeautifulSoup\n",
      "from sets import Set\n",
      "from sgmllib import SGMLParser\n",
      "import pickle "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cite: https://github.com/chungy/diveintopython/blob/master/py/urllister.py\n",
      "class URLLister(SGMLParser):\n",
      "    \"\"\"\n",
      "    Arguments: (implicit) SGMLParser \n",
      "    \n",
      "    Extracts all 'href's from an html document. We store these links in a set, \n",
      "    and process them individually to retrieve content or additional links. \n",
      "    \"\"\"\n",
      "    def reset(self):\n",
      "        SGMLParser.reset(self)\n",
      "        self.urls = []\n",
      "\n",
      "    def start_a(self, attrs):\n",
      "        href = [v for k, v in attrs if k=='href']\n",
      "        if href:\n",
      "            self.urls.extend(href)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_url(url, url_mod, string_start, find_string, start_article_delim, end_article_delim, name, pol):\n",
      "    \"\"\"\n",
      "    Arguments: string url, string url_mod, string find_string, string start_title_delim, \n",
      "    string end_title_delim, string start_article_delim, string end_article_delim, string name, string pol\n",
      "    \n",
      "    For each url passed to process_url, open the CSV file articles.csv which contains all data scraped.\n",
      "    Process the article by retrieving the title, and by using BeautifulSoup to retrieve the article text.\n",
      "    Store each article in the CSV as the publication name, the publication's political leaning (either 'L' or 'C'), article title, and article content.  \n",
      "    \"\"\"\n",
      "        \n",
      "    \n",
      "    #with open('articles_scraped.pickle', 'a+') as handle:\n",
      "    #    pickle.dump(set(), handle)\n",
      "    \n",
      "    # check url format\n",
      "    if len(url) > 0 and ((str(url)[0] == '/' and string_start == \"\") or str(url).find(string_start) == 0) and str(url).find(find_string) != -1: \n",
      "        with open('articles.csv', 'a+') as csvfile:\n",
      "            article_writer = csv.writer(csvfile, delimiter=',')\n",
      "            \n",
      "            tmp_url = url_mod + str(url)\n",
      "            usock = urllib.urlopen(tmp_url)\n",
      "            html = usock.read()\n",
      "                        \n",
      "            # use Beautiful Soup \n",
      "            soup_unprocessed = BeautifulSoup(html)\n",
      "              \n",
      "            # use try/except to catch cases in which articles do not conform to general standards\n",
      "            # e.g. when an article does not have a title\n",
      "            try:\n",
      "                title = soup_unprocessed.title.string \n",
      "                title = title.encode(\"utf-8\")\n",
      "                \n",
      "                 # If article is from a specific provider, modify it according to the following rules: \n",
      "                # e.g. in the case of Townhall, we only want html which has the full article rather than a paged version\n",
      "                if name == \"TownHall\" and html.find(\"View Full Article\") != -1:\n",
      "                    process_url(url + '/page/full', url_mod, string_start, find_string, start_article_delim, end_article_delim, name, pol)\n",
      "                    usock.close()\n",
      "                    return\n",
      "                elif name == \"Slate\" and html.find('<div class=\"single-page\">') != -1: \n",
      "                    process_url(url[0:url.find('.html')] + '.single.html', url_mod, string_start, find_string, start_article_delim, end_article_delim, name, pol)\n",
      "                    usock.close()\n",
      "                    return\n",
      "                elif name == \"Mother Jones\" and (title.find('Issue') != -1 or title.find('map') != -1):\n",
      "                    print 'Content not text, or article not relevant.\\n'\n",
      "                    usock.close()\n",
      "                    return\n",
      "                elif name == \"The American Conservative\" and html.find('Author Archives') != -1 or title.find('Web Headlines') != -1 or title.find('Articles') != -1:\n",
      "                    print 'Content not text, or article not relevant.\\n'\n",
      "                    usock.close()\n",
      "                    return\n",
      "                elif name == \"The Christian Science Monitor\" and (title.find('+video') != -1 or url.find('The-Culture') != -1 or title.find('Photos of the day') != -1 or title.find('How much do you know about') != -1):\n",
      "                    print 'Content not text, or article not relevant.\\n'\n",
      "                    usock.close()\n",
      "                    return \n",
      "                elif name == \"The Blaze\" and title.find('Video') != -1:\n",
      "                    print 'Content not text, or article not relevant.\\n'\n",
      "                    usock.close()\n",
      "                    return\n",
      "                elif name == \"Slate\" and (url.find('video') != -1 or url.find('podcast') != -1): \n",
      "                    print 'Content not text, or article not relevant.\\n'\n",
      "                    usock.close()\n",
      "                    return\n",
      "                elif name == \"The Washington Post\" and (url.find('live') != -1 or html.find('posttv-video-template') != -1):\n",
      "                    print 'Content not text, or article not relevant.\\n'\n",
      "                    usock.close()\n",
      "                    return\n",
      "                else: \n",
      "                    print title + '\\n'\n",
      "                    content = str(html)[str(html).find(start_article_delim) + len (start_article_delim):str(html).find(end_article_delim)]\n",
      "                    soup = BeautifulSoup(content)\n",
      "                    mod_content = soup.get_text()\n",
      "                \n",
      "                with open('articles_scraped.pickle', 'rb') as handle:\n",
      "                    article_list = pickle.load(handle)\n",
      "                    if title in article_list: \n",
      "                        print '\\n ALREADY IN SET \\n'\n",
      "                        return\n",
      "                    else:\n",
      "                        with open('articles_scraped.pickle', 'wb') as handle:\n",
      "                            article_list.append(title)\n",
      "                            pickle.dump(article_list, handle)\n",
      "                \n",
      "                mod_content = mod_content.encode(\"utf-8\") \n",
      "                \n",
      "                ascii_title = unicode(title, 'ascii', 'ignore')\n",
      "                ascii_content = unicode(mod_content, 'ascii', 'ignore')\n",
      "\n",
      "                article_writer.writerow([name, pol, ascii_title, ascii_content])\n",
      "            except AttributeError: \n",
      "                print 'Attribute Error'\n",
      "                \n",
      "            usock.close()\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape(start_url, iterator, url_list, url_mod):      \n",
      "    \"\"\"\n",
      "    Arguments: string start_url, int iterator, set url_list, string url_mod\n",
      "    Returns: updated set url_list \n",
      "    \n",
      "    For each start_url passed, this function opens that url and retrieves all hrefs from that page. \n",
      "    If iterator is smaller than the (constant) number of ITER_PAGES, scrape is called recursively \n",
      "    on the set of urls. \n",
      "    \"\"\"\n",
      "    \n",
      "    # limit the number of articles retrieved by limiting page depth\n",
      "    if iterator > ITER_PAGES: \n",
      "        return set()\n",
      "    \n",
      "    # call URLLister to retrieve all hrefs in the html read from the url \n",
      "    usock = urllib.urlopen(start_url)\n",
      "    parser = URLLister()\n",
      "    html = usock.read()\n",
      "    parser.feed(html)\n",
      "    usock.close()\n",
      "    parser.close()\n",
      "    \n",
      "    url_list = url_list.union(parser.urls)\n",
      "    \n",
      "    # scrape each correctly formatted url\n",
      "    for url in url_list: \n",
      "        if len(url) > 0 and str(url)[0] == '/': \n",
      "            tmp_url = url_mod + str(url)\n",
      "            try: \n",
      "                tmp_set = scrape(tmp_url, iterator + 1, url_list, url_mod)\n",
      "                url_list = url_list.union(tmp_set) \n",
      "            except IOError: \n",
      "                print \"Couldn't Open\"\n",
      "    return url_list\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ITER_PAGES = 1\n",
      "\n",
      "# pickle setup\n",
      "articles_to_pickle = []\n",
      "try:\n",
      "    with open('articles_scraped.pickle', 'rb') as handle:\n",
      "        articles_to_pickle = pickle.load(handle)\n",
      "except IOError: \n",
      "    with open('articles_scraped.pickle', 'wb') as handle:\n",
      "        pickle.dump(articles_to_pickle, handle)\n",
      "\n",
      "# call the function\n",
      "url_list = set()  \n",
      "american_conservative_list = scrape('http://www.theamericanconservative.com/', 1, url_list, '')\n",
      "townhall_list = scrape('http://www.townhall.com/', 1, url_list, 'http://www.townhall.com')\n",
      "csmonitor_list = scrape('http://www.csmonitor.com/', 1, url_list, 'http://www.csmonitor.com/')\n",
      "weekly_std_list = scrape('http://www.weeklystandard.com/', 1, url_list, 'http://www.weeklystandard.com/')\n",
      "spectator_list = scrape('http://spectator.org/', 1, url_list, 'http://spectator.org/')\n",
      "\n",
      "mother_jones_list = scrape('http://www.motherjones.com/', 1, url_list, 'http://www.motherjones.com')\n",
      "nation_list = scrape('http://www.thenation.com/', 1, url_list, 'http://www.thenation.com/')\n",
      "slate_list = scrape('http://www.slate.com/', 1, url_list, '')\n",
      "newyorker_list = scrape('http://www.newyorker.com/', 1, url_list, '')\n",
      "wp_list = scrape('http://www.washingtonpost.com/', 1, url_list, '')\n",
      "\n",
      "print '\\nurls\\n'\n",
      "\n",
      "# conservative 1\n",
      "for url in american_conservative_list: \n",
      "    process_url(url, '', 'http://www.theamericanconservative.com/', 'theamericanconservative', '<div class=\"post-content\">', '<footer id=\"articlefooter\">', 'The American Conservative', 'C')\n",
      "\n",
      "# conservative 2\n",
      "for url in townhall_list: \n",
      "    process_url(url, 'http://www.townhall.com', 'argument-not-used', '2014' , '<ul class=\"breadcrumb\">', '<hr class=\"article-divider\"', 'TownHall', 'C')\n",
      "\n",
      "# conservative 3\n",
      "for url in csmonitor_list:\n",
      "    process_url(url, 'http://www.csmonitor.com/', 'argument-not-used', '2014/', '<div id=\"story-body\"', '<span id=\"end-of-story\"', 'The Christian Science Monitor', 'C')\n",
      "\n",
      "# conservative 4\n",
      "for url in weekly_std_list: \n",
      "    process_url(url, 'http://www.weeklystandard.com/', 'argument-not-used', '/articles/', '  <div class=\"all_article\">', '<div class=\"article-footer\">', 'The Weekly Standard', 'C')\n",
      "\n",
      "# conservative 5\n",
      "for url in spectator_list:\n",
      "    process_url(url, 'http://spectator.org/', 'argument-not-used', '/articles/', 'target=\"_blank\" rel=\"nofollow\">', '</iframe></div><div class=\"field-item even\"><p class=\"label\">', 'The American Spectator', 'C')\n",
      "\n",
      "# liberal 1 \n",
      "for url in mother_jones_list:\n",
      "    process_url(url, 'http://www.motherjones.com', 'argument-not-used', '2014' , '<div id=\"node-header\" class=\"clear-block\">', '<div id=\"node-footer\" class=\"clear-block\">', 'Mother Jones', 'L')\n",
      "\n",
      "# liberal 2\n",
      "for url in nation_list:\n",
      "    process_url(url, 'http://www.thenation.com/', 'argument-not-used', '/article/', '<div class=\"field field-type-text field-field-image-caption\">', '</p></div><div class=\"views-field-value byline\">', 'The Nation', 'L')\n",
      "\n",
      "# liberal 3\n",
      "for url in slate_list:\n",
      "    process_url(url, '', 'http://www.slate.com/', '/2014/', '<div class=\"text text-1 parbase section\">', '<section class=\"about-the-author', 'Slate', 'L')\n",
      "\n",
      "# liberal 4 - slightly more complex \n",
      "for url in newyorker_list:\n",
      "    if url.find('//') == 0 or len(url) < 45:\n",
      "        print 'URL badly formatted'\n",
      "        # do nothing for this url\n",
      "    else:\n",
      "        process_url(url, '', 'http://www.newyorker.com/', 'news', '<div itemprop=\"articleBody\" class=\"articleBody\" id=\"articleBody\">', '<span class=\"dingbat\">', 'The New Yorker', 'L')\n",
      "        process_url(url, '', 'http://www.newyorker.com/', 'magazine/2014', '<div itemprop=\"articleBody\" class=\"articleBody\" id=\"articleBody\">', '<span class=\"dingbat\">', 'The New Yorker', 'L')\n",
      "\n",
      "# liberal 5\n",
      "for url in wp_list:\n",
      "    print url\n",
      "    process_url(url, '', 'http://www.washingtonpost.com/', '/2014/', '<div id=\"article-body\" class=\"article-body\">', '</article>', 'The Washington Post', 'L')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "urls\n",
        "\n",
        "What Not to Do on Iran | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Content not text, or article not relevant.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "UVA\u2019s Jackie: A Wahoo Tawana Brawley? | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Content not text, or article not relevant.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Channing Tatum. Wrestling Picture. Whadaya Need \u2013 A Road Map? | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Europe\u2019s Conchita Wurst Consensus | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Content not text, or article not relevant.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Content not text, or article not relevant.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Double Feature Feature: 10,000 Hours Edition | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "What Are \u201cNational Rights\u201d? | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "About Us | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Grand Strategy Is Bunk | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "A Fraternity of Rape | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "RIP The New Republic | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Bringing Back Husbandry | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Content not text, or article not relevant.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Is Clickbait the End of Online News? | The American Conservative\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Content not text, or article not relevant.\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Part 2: Feature Vectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}